{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLCXm8IsSS2"
      },
      "source": [
        "# Download the Cora data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xRN47p1SKRgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8a9400-a483-44c8-e3a1-040c3ee89779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 03:09:43--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
            "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
            "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168052 (164K) [application/x-gzip]\n",
            "Saving to: ‘cora.tgz.1’\n",
            "\n",
            "cora.tgz.1          100%[===================>] 164.11K  1002KB/s    in 0.2s    \n",
            "\n",
            "2023-04-11 03:09:44 (1002 KB/s) - ‘cora.tgz.1’ saved [168052/168052]\n",
            "\n",
            "cora/\n",
            "cora/README\n",
            "cora/cora.cites\n",
            "cora/cora.content\n"
          ]
        }
      ],
      "source": [
        "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "! tar -zxvf cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYzURA4OKg"
      },
      "source": [
        "# import modules and set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "uJQYMX02_z0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "seed = 0\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOv1h7YsK-5"
      },
      "source": [
        "# Loading and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kXPHN61i9keB"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    # Random indexes\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    # Nodes for training\n",
        "    idx_train = idx_rand[:training_samples]\n",
        "    # Nodes for validation\n",
        "    idx_val= idx_rand[training_samples:]\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"symmetric normalization\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzCZVd1JsbHr"
      },
      "source": [
        "## check the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlsKjMKx8_b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bce5cda-d341-4b83-f1e7-7f04565880b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "adj, features, labels, idx_train, idx_val = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxrv21rLnpiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce5b332-d4e3-4fef-8672-ea734a3bf355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
            "torch.Size([2708, 2708])\n"
          ]
        }
      ],
      "source": [
        "print(adj)\n",
        "print(adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWrDf0iWnpqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c001f267-2aef-4e3f-d36b-bc088372998a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([2708, 1433])\n"
          ]
        }
      ],
      "source": [
        "print(features)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUkt2JJdsuA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f4134b-7728-4a9b-ed99-b2da26015ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "2708\n"
          ]
        }
      ],
      "source": [
        "print(labels)\n",
        "print(labels.unique())\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGP18jNAs1Gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76be9e5d-e9cc-4d43-f922-f1eaeb5ac72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n",
            "2568\n"
          ]
        }
      ],
      "source": [
        "print(len(idx_train))\n",
        "print(len(idx_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqIcfH-vIic"
      },
      "source": [
        "# Vanilla GCN for node classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Graph Convolution layer (Your Task)\n",
        "\n",
        "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
        "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
        "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
      ],
      "metadata": {
        "id": "f48tylWyjLPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-fU8L7f41VZ"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    A Graph Convolution Layer (GCN)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        \"\"\"\n",
        "        * `in_features`, $F$, is the number of input features per node\n",
        "        * `out_features`, $F'$, is the number of output features per node\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
        "        # hint: use nn.Linear()\n",
        "        ############ Your code here ###################################\n",
        "        self.W = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
        "        # to sum over neighbouring nodes )\n",
        "        # hint: use the linear layer you declared above. \n",
        "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
        "        #       adjacency matrix\n",
        "        ############ Your code here ###################################\n",
        "        s = self.W(input)\n",
        "        h_prime = torch.spmm(adj, s)\n",
        "        return h_prime\n",
        "        ###############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GCN (Your Task)\n",
        "\n",
        "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
      ],
      "metadata": {
        "id": "RxBELCxkjF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    '''\n",
        "    A two-layer GCN\n",
        "    '''\n",
        "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
        "        \"\"\"\n",
        "        * `nfeat`, is the number of input features per node of the first layer\n",
        "        * `n_hidden`, number of hidden units\n",
        "        * `n_classes`, total number of classes for classification\n",
        "        * `dropout`, the dropout ratio\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "        # TODO: Initialization\n",
        "        # (1) 2 GraphConvolution() layers. \n",
        "        # (2) 1 Dropout layer\n",
        "        # (3) 1 activation function: ReLU()\n",
        "        ############ Your code here ###################################\n",
        "        self.conv1 = GraphConvolution(nfeat, n_hidden, bias)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = GraphConvolution(n_hidden, n_classes, bias)\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # TODO: the input will pass through the first graph convolution layer, \n",
        "        # the activation function, the dropout layer, then the second graph \n",
        "        # convolution layer. No activation function for the \n",
        "        # last layer. Return the logits. \n",
        "        ############ Your code here ###################################\n",
        "        h = self.conv1(x, adj)\n",
        "        h = self.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        out = self.conv2(h, adj)\n",
        "        logits = F.softmax(out, dim=0)\n",
        "        return logits\n",
        "        ###############################################################"
      ],
      "metadata": {
        "id": "HtVr2cN8jD5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1d9F1G508r"
      },
      "source": [
        "## define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyhqJ39OCzNN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXsdid6C5K1c"
      },
      "source": [
        "## training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlYeoFPFAWm"
      },
      "outputs": [],
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbx0uc-9G5vs"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjNiui83FYBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db3d1b6-fd78-45be-d614-7f58ac719750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GCN(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Vanilla GCN"
      ],
      "metadata": {
        "id": "1W6tqqj16iz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSjUYJPSlnOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f97664c-5057-43b0-b1c6-c2890efdb67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9459 acc_train: 0.0929 loss_val: 1.9459 acc_val: 0.1293 time: 2.9250s\n",
            "Epoch: 0002 loss_train: 1.9459 acc_train: 0.0857 loss_val: 1.9459 acc_val: 0.1266 time: 0.0034s\n",
            "Epoch: 0003 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.1258 time: 0.0034s\n",
            "Epoch: 0004 loss_train: 1.9459 acc_train: 0.0857 loss_val: 1.9459 acc_val: 0.1254 time: 0.0034s\n",
            "Epoch: 0005 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.1285 time: 0.0034s\n",
            "Epoch: 0006 loss_train: 1.9459 acc_train: 0.0714 loss_val: 1.9459 acc_val: 0.1266 time: 0.0034s\n",
            "Epoch: 0007 loss_train: 1.9459 acc_train: 0.0929 loss_val: 1.9459 acc_val: 0.1269 time: 0.0034s\n",
            "Epoch: 0008 loss_train: 1.9459 acc_train: 0.1000 loss_val: 1.9459 acc_val: 0.1285 time: 0.0034s\n",
            "Epoch: 0009 loss_train: 1.9459 acc_train: 0.0857 loss_val: 1.9459 acc_val: 0.1246 time: 0.0034s\n",
            "Epoch: 0010 loss_train: 1.9459 acc_train: 0.0857 loss_val: 1.9459 acc_val: 0.1242 time: 0.0034s\n",
            "Epoch: 0011 loss_train: 1.9459 acc_train: 0.0857 loss_val: 1.9459 acc_val: 0.2305 time: 0.0034s\n",
            "Epoch: 0012 loss_train: 1.9459 acc_train: 0.2071 loss_val: 1.9459 acc_val: 0.2438 time: 0.0034s\n",
            "Epoch: 0013 loss_train: 1.9459 acc_train: 0.2143 loss_val: 1.9459 acc_val: 0.2445 time: 0.0034s\n",
            "Epoch: 0014 loss_train: 1.9459 acc_train: 0.2000 loss_val: 1.9459 acc_val: 0.2465 time: 0.0034s\n",
            "Epoch: 0015 loss_train: 1.9459 acc_train: 0.2000 loss_val: 1.9459 acc_val: 0.2457 time: 0.0034s\n",
            "Epoch: 0016 loss_train: 1.9459 acc_train: 0.2071 loss_val: 1.9459 acc_val: 0.2445 time: 0.0047s\n",
            "Epoch: 0017 loss_train: 1.9459 acc_train: 0.2071 loss_val: 1.9459 acc_val: 0.2461 time: 0.0034s\n",
            "Epoch: 0018 loss_train: 1.9459 acc_train: 0.2071 loss_val: 1.9459 acc_val: 0.2453 time: 0.0034s\n",
            "Epoch: 0019 loss_train: 1.9459 acc_train: 0.1786 loss_val: 1.9459 acc_val: 0.1519 time: 0.0034s\n",
            "Epoch: 0020 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1328 time: 0.0034s\n",
            "Epoch: 0021 loss_train: 1.9459 acc_train: 0.0643 loss_val: 1.9459 acc_val: 0.1359 time: 0.0034s\n",
            "Epoch: 0022 loss_train: 1.9459 acc_train: 0.0643 loss_val: 1.9459 acc_val: 0.1375 time: 0.0034s\n",
            "Epoch: 0023 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.1367 time: 0.0034s\n",
            "Epoch: 0024 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.0927 time: 0.0034s\n",
            "Epoch: 0025 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.0915 time: 0.0034s\n",
            "Epoch: 0026 loss_train: 1.9459 acc_train: 0.0786 loss_val: 1.9459 acc_val: 0.0954 time: 0.0034s\n",
            "Epoch: 0027 loss_train: 1.9459 acc_train: 0.0929 loss_val: 1.9459 acc_val: 0.1301 time: 0.0053s\n",
            "Epoch: 0028 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1293 time: 0.0034s\n",
            "Epoch: 0029 loss_train: 1.9459 acc_train: 0.1429 loss_val: 1.9459 acc_val: 0.1293 time: 0.0034s\n",
            "Epoch: 0030 loss_train: 1.9459 acc_train: 0.1429 loss_val: 1.9459 acc_val: 0.1269 time: 0.0034s\n",
            "Epoch: 0031 loss_train: 1.9459 acc_train: 0.1286 loss_val: 1.9459 acc_val: 0.1289 time: 0.0034s\n",
            "Epoch: 0032 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1121 time: 0.0034s\n",
            "Epoch: 0033 loss_train: 1.9459 acc_train: 0.1214 loss_val: 1.9459 acc_val: 0.1390 time: 0.0034s\n",
            "Epoch: 0034 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1410 time: 0.0035s\n",
            "Epoch: 0035 loss_train: 1.9459 acc_train: 0.1429 loss_val: 1.9459 acc_val: 0.1406 time: 0.0035s\n",
            "Epoch: 0036 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1406 time: 0.0069s\n",
            "Epoch: 0037 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1414 time: 0.0066s\n",
            "Epoch: 0038 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1414 time: 0.0051s\n",
            "Epoch: 0039 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1394 time: 0.0048s\n",
            "Epoch: 0040 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1390 time: 0.0034s\n",
            "Epoch: 0041 loss_train: 1.9459 acc_train: 0.1714 loss_val: 1.9459 acc_val: 0.1386 time: 0.0034s\n",
            "Epoch: 0042 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1390 time: 0.0030s\n",
            "Epoch: 0043 loss_train: 1.9459 acc_train: 0.1643 loss_val: 1.9459 acc_val: 0.1398 time: 0.0028s\n",
            "Epoch: 0044 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1398 time: 0.0028s\n",
            "Epoch: 0045 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1390 time: 0.0032s\n",
            "Epoch: 0046 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1390 time: 0.0046s\n",
            "Epoch: 0047 loss_train: 1.9459 acc_train: 0.1643 loss_val: 1.9459 acc_val: 0.1402 time: 0.0028s\n",
            "Epoch: 0048 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1398 time: 0.0028s\n",
            "Epoch: 0049 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1406 time: 0.0028s\n",
            "Epoch: 0050 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1402 time: 0.0028s\n",
            "Epoch: 0051 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1398 time: 0.0028s\n",
            "Epoch: 0052 loss_train: 1.9459 acc_train: 0.1643 loss_val: 1.9459 acc_val: 0.1394 time: 0.0028s\n",
            "Epoch: 0053 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1394 time: 0.0028s\n",
            "Epoch: 0054 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1390 time: 0.0028s\n",
            "Epoch: 0055 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1382 time: 0.0028s\n",
            "Epoch: 0056 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1379 time: 0.0028s\n",
            "Epoch: 0057 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1379 time: 0.0028s\n",
            "Epoch: 0058 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1382 time: 0.0028s\n",
            "Epoch: 0059 loss_train: 1.9459 acc_train: 0.1500 loss_val: 1.9459 acc_val: 0.1394 time: 0.0028s\n",
            "Epoch: 0060 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.1402 time: 0.0028s\n",
            "Epoch: 0061 loss_train: 1.9459 acc_train: 0.1571 loss_val: 1.9459 acc_val: 0.2305 time: 0.0028s\n",
            "Epoch: 0062 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2309 time: 0.0028s\n",
            "Epoch: 0063 loss_train: 1.9459 acc_train: 0.2500 loss_val: 1.9459 acc_val: 0.2309 time: 0.0028s\n",
            "Epoch: 0064 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2309 time: 0.0028s\n",
            "Epoch: 0065 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2294 time: 0.0028s\n",
            "Epoch: 0066 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2294 time: 0.0028s\n",
            "Epoch: 0067 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0068 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0069 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0070 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0029s\n",
            "Epoch: 0071 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0072 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0073 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0074 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0075 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2301 time: 0.0028s\n",
            "Epoch: 0076 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0077 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0078 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0079 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2298 time: 0.0046s\n",
            "Epoch: 0080 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2298 time: 0.0029s\n",
            "Epoch: 0081 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0028s\n",
            "Epoch: 0082 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2290 time: 0.0029s\n",
            "Epoch: 0083 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2290 time: 0.0029s\n",
            "Epoch: 0084 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2294 time: 0.0026s\n",
            "Epoch: 0085 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0086 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2298 time: 0.0026s\n",
            "Epoch: 0087 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0088 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2286 time: 0.0025s\n",
            "Epoch: 0089 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0024s\n",
            "Epoch: 0090 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0091 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2286 time: 0.0024s\n",
            "Epoch: 0092 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0093 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0094 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0095 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0096 loss_train: 1.9459 acc_train: 0.2357 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0097 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0098 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0025s\n",
            "Epoch: 0099 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0043s\n",
            "Epoch: 0100 loss_train: 1.9459 acc_train: 0.2429 loss_val: 1.9459 acc_val: 0.2298 time: 0.0088s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4401s\n",
            "Test set results: loss= 1.9459 accuracy= 0.2298\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# evaluating\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF3eM6DhHfE_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCFwzVLmPXnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks"
      ],
      "metadata": {
        "id": "mKHEyXp1EVdo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx15HdotKnt_"
      },
      "source": [
        "## Graph attention layer (Your task)\n",
        "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
        "\n",
        "\n",
        "### The initial transformation\n",
        "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
        "\n",
        "### attention score\n",
        "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
        "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
        "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
        "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
        "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
        "\n",
        "#### How to vectorize this? Some hints: \n",
        "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
        "\n",
        "2. `tensor.repeat_interleave()` gives you\n",
        "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
        "\n",
        "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
        "\n",
        "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
        "\n",
        "\n",
        "#### Perform softmax \n",
        "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
        "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
        "\n",
        "#### Apply dropout\n",
        "Apply the dropout layer. (this step is easy)\n",
        "\n",
        "#### Calculate final output for each head\n",
        "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
        "\n",
        "\n",
        "#### Concat or Mean\n",
        "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "wVu7rcOuAUZz"
      },
      "outputs": [],
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
        "                 is_concat: bool = True,\n",
        "                 dropout: float = 0.6,\n",
        "                 alpha: float = 0.2):\n",
        "        \"\"\"\n",
        "        in_features: F, the number of input features per node\n",
        "        out_features: F', the number of output features per node\n",
        "        n_heads: K, the number of attention heads\n",
        "        is_concat: whether the multi-head results should be concatenated or averaged\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative slope for leaky relu activation\n",
        "        \"\"\"\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        if is_concat:\n",
        "            assert out_features % n_heads == 0\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else:\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        # TODO: initialize the following modules: \n",
        "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
        "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
        "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
        "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
        "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
        "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
        "        ################ your code here ########################\n",
        "        self.W = nn.Linear(in_features, self.n_heads * self.n_hidden, bias=False)\n",
        "        self.attention = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=alpha)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        ########################################################\n",
        "\n",
        "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        # Number of nodes\n",
        "        n_nodes = h.shape[0]\n",
        "        \n",
        "        # TODO: \n",
        "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
        "        #     (you can use tensor.view() function)\n",
        "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
        "        # (3) apply the attention layer \n",
        "        # (4) apply the activation layer (you will get the attention score e)\n",
        "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
        "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
        "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
        "        # (7) apply softmax \n",
        "        # (8) apply dropout_layer \n",
        "        ############## Your code here #########################################\n",
        "        s = self.W(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
        "        s_cat = torch.cat([s.unsqueeze(1).repeat_interleave(n_nodes, dim=1), s.unsqueeze(0).repeat_interleave(n_nodes, dim=0)], dim=-1)\n",
        "        att = self.attention(s_cat)\n",
        "        e = self.activation(att)\n",
        "        e = e.squeeze(dim=-1)\n",
        "        e[adj_mat.unsqueeze(-1).expand(-1, -1, self.n_heads) == 0] = -math.inf\n",
        "        a = self.softmax(e)\n",
        "        a = self.dropout_layer(a)\n",
        "        #######################################################################\n",
        "\n",
        "        # Summation \n",
        "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
        "\n",
        "\n",
        "        # TODO: Concat or Mean\n",
        "        # Concatenate the heads\n",
        "        if self.is_concat:\n",
        "            ############## Your code here #########################################\n",
        "            h_prime = h_prime.contiguous().view(n_nodes, -1)\n",
        "\n",
        "            #######################################################################\n",
        "        # Take the mean of the heads (for the last layer)\n",
        "        else:\n",
        "            ############## Your code here #########################################\n",
        "            h_prime = h_prime.mean(dim=1)\n",
        "\n",
        "        return h_prime\n",
        "            #######################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GAT network\n",
        "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
      ],
      "metadata": {
        "id": "YOSk_ZShi2nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
        "        \"\"\"\n",
        "        in_features: the number of features per node\n",
        "        n_hidden: the number of features in the first graph attention layer\n",
        "        n_classes: the number of classes\n",
        "        n_heads: the number of heads in the graph attention layers\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First graph attention layer where we concatenate the heads\n",
        "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
        "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
        "        self.activation = nn.ELU()  \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: the features vectors\n",
        "        adj_mat: the adjacency matrix\n",
        "        \"\"\"\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc1(x, adj_mat)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc2(x, adj_mat)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKNbUtPVi1Vs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training GAT"
      ],
      "metadata": {
        "id": "CtRQ3Ced7RAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        \"alpha\": 0.2,\n",
        "        \"n_heads\": 8\n",
        "        }"
      ],
      "metadata": {
        "id": "b7D5mYXC6zTG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "7MYaK98hDy7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecc7997-b5c4-421a-edb8-54f8825de816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GAT(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"],\n",
        "            alpha=args[\"alpha\"],\n",
        "            n_heads=args[\"n_heads\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "E9FcfXwMDzEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d15bc1-5d5f-4b50-a028-7a0f1edc5c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9461 acc_train: 0.1429 loss_val: 1.9431 acc_val: 0.5175 time: 0.3300s\n",
            "Epoch: 0002 loss_train: 1.9420 acc_train: 0.5000 loss_val: 1.9401 acc_val: 0.5109 time: 0.2623s\n",
            "Epoch: 0003 loss_train: 1.9364 acc_train: 0.5571 loss_val: 1.9368 acc_val: 0.4883 time: 0.2625s\n",
            "Epoch: 0004 loss_train: 1.9326 acc_train: 0.5571 loss_val: 1.9331 acc_val: 0.4766 time: 0.2624s\n",
            "Epoch: 0005 loss_train: 1.9241 acc_train: 0.5571 loss_val: 1.9291 acc_val: 0.4568 time: 0.2638s\n",
            "Epoch: 0006 loss_train: 1.9178 acc_train: 0.5714 loss_val: 1.9248 acc_val: 0.4412 time: 0.2636s\n",
            "Epoch: 0007 loss_train: 1.9098 acc_train: 0.5786 loss_val: 1.9202 acc_val: 0.4330 time: 0.2619s\n",
            "Epoch: 0008 loss_train: 1.9060 acc_train: 0.5571 loss_val: 1.9153 acc_val: 0.4229 time: 0.2627s\n",
            "Epoch: 0009 loss_train: 1.8980 acc_train: 0.5857 loss_val: 1.9100 acc_val: 0.4147 time: 0.2629s\n",
            "Epoch: 0010 loss_train: 1.8867 acc_train: 0.5286 loss_val: 1.9043 acc_val: 0.4065 time: 0.2648s\n",
            "Epoch: 0011 loss_train: 1.8767 acc_train: 0.5357 loss_val: 1.8983 acc_val: 0.4019 time: 0.2638s\n",
            "Epoch: 0012 loss_train: 1.8736 acc_train: 0.5071 loss_val: 1.8919 acc_val: 0.3964 time: 0.2637s\n",
            "Epoch: 0013 loss_train: 1.8543 acc_train: 0.5571 loss_val: 1.8851 acc_val: 0.3929 time: 0.2634s\n",
            "Epoch: 0014 loss_train: 1.8322 acc_train: 0.5571 loss_val: 1.8779 acc_val: 0.3879 time: 0.2628s\n",
            "Epoch: 0015 loss_train: 1.8384 acc_train: 0.5571 loss_val: 1.8703 acc_val: 0.3843 time: 0.2629s\n",
            "Epoch: 0016 loss_train: 1.8261 acc_train: 0.5071 loss_val: 1.8624 acc_val: 0.3836 time: 0.2623s\n",
            "Epoch: 0017 loss_train: 1.8104 acc_train: 0.5857 loss_val: 1.8541 acc_val: 0.3828 time: 0.2626s\n",
            "Epoch: 0018 loss_train: 1.7916 acc_train: 0.5357 loss_val: 1.8454 acc_val: 0.3805 time: 0.2636s\n",
            "Epoch: 0019 loss_train: 1.7730 acc_train: 0.5071 loss_val: 1.8363 acc_val: 0.3789 time: 0.2624s\n",
            "Epoch: 0020 loss_train: 1.7709 acc_train: 0.4786 loss_val: 1.8269 acc_val: 0.3758 time: 0.2625s\n",
            "Epoch: 0021 loss_train: 1.7505 acc_train: 0.5071 loss_val: 1.8173 acc_val: 0.3750 time: 0.2627s\n",
            "Epoch: 0022 loss_train: 1.7203 acc_train: 0.5000 loss_val: 1.8076 acc_val: 0.3742 time: 0.2632s\n",
            "Epoch: 0023 loss_train: 1.7196 acc_train: 0.5714 loss_val: 1.7976 acc_val: 0.3727 time: 0.2634s\n",
            "Epoch: 0024 loss_train: 1.6667 acc_train: 0.5429 loss_val: 1.7872 acc_val: 0.3707 time: 0.2635s\n",
            "Epoch: 0025 loss_train: 1.6996 acc_train: 0.5286 loss_val: 1.7769 acc_val: 0.3688 time: 0.2628s\n",
            "Epoch: 0026 loss_train: 1.6794 acc_train: 0.5143 loss_val: 1.7665 acc_val: 0.3684 time: 0.2625s\n",
            "Epoch: 0027 loss_train: 1.6351 acc_train: 0.5571 loss_val: 1.7559 acc_val: 0.3676 time: 0.2626s\n",
            "Epoch: 0028 loss_train: 1.6116 acc_train: 0.4786 loss_val: 1.7454 acc_val: 0.3660 time: 0.2628s\n",
            "Epoch: 0029 loss_train: 1.6061 acc_train: 0.5214 loss_val: 1.7348 acc_val: 0.3657 time: 0.2624s\n",
            "Epoch: 0030 loss_train: 1.6088 acc_train: 0.5286 loss_val: 1.7241 acc_val: 0.3653 time: 0.2628s\n",
            "Epoch: 0031 loss_train: 1.5972 acc_train: 0.5143 loss_val: 1.7135 acc_val: 0.3660 time: 0.2626s\n",
            "Epoch: 0032 loss_train: 1.5827 acc_train: 0.4643 loss_val: 1.7031 acc_val: 0.3668 time: 0.2628s\n",
            "Epoch: 0033 loss_train: 1.5493 acc_train: 0.4571 loss_val: 1.6927 acc_val: 0.3684 time: 0.2627s\n",
            "Epoch: 0034 loss_train: 1.5655 acc_train: 0.5429 loss_val: 1.6824 acc_val: 0.3746 time: 0.2636s\n",
            "Epoch: 0035 loss_train: 1.4933 acc_train: 0.5071 loss_val: 1.6721 acc_val: 0.3801 time: 0.2634s\n",
            "Epoch: 0036 loss_train: 1.4944 acc_train: 0.4643 loss_val: 1.6619 acc_val: 0.3855 time: 0.2638s\n",
            "Epoch: 0037 loss_train: 1.5687 acc_train: 0.4786 loss_val: 1.6518 acc_val: 0.3949 time: 0.2626s\n",
            "Epoch: 0038 loss_train: 1.4273 acc_train: 0.5714 loss_val: 1.6416 acc_val: 0.4058 time: 0.2627s\n",
            "Epoch: 0039 loss_train: 1.3983 acc_train: 0.5571 loss_val: 1.6313 acc_val: 0.4167 time: 0.2624s\n",
            "Epoch: 0040 loss_train: 1.5218 acc_train: 0.5000 loss_val: 1.6212 acc_val: 0.4303 time: 0.2617s\n",
            "Epoch: 0041 loss_train: 1.4290 acc_train: 0.5643 loss_val: 1.6111 acc_val: 0.4482 time: 0.2623s\n",
            "Epoch: 0042 loss_train: 1.4157 acc_train: 0.5500 loss_val: 1.6010 acc_val: 0.4657 time: 0.2626s\n",
            "Epoch: 0043 loss_train: 1.3713 acc_train: 0.5929 loss_val: 1.5910 acc_val: 0.4778 time: 0.2630s\n",
            "Epoch: 0044 loss_train: 1.3935 acc_train: 0.6071 loss_val: 1.5810 acc_val: 0.4930 time: 0.2628s\n",
            "Epoch: 0045 loss_train: 1.3567 acc_train: 0.6286 loss_val: 1.5711 acc_val: 0.5051 time: 0.2628s\n",
            "Epoch: 0046 loss_train: 1.3814 acc_train: 0.6143 loss_val: 1.5612 acc_val: 0.5218 time: 0.2630s\n",
            "Epoch: 0047 loss_train: 1.3390 acc_train: 0.6143 loss_val: 1.5513 acc_val: 0.5308 time: 0.2635s\n",
            "Epoch: 0048 loss_train: 1.3629 acc_train: 0.6214 loss_val: 1.5414 acc_val: 0.5436 time: 0.2638s\n",
            "Epoch: 0049 loss_train: 1.3127 acc_train: 0.6429 loss_val: 1.5318 acc_val: 0.5561 time: 0.2653s\n",
            "Epoch: 0050 loss_train: 1.3306 acc_train: 0.6929 loss_val: 1.5221 acc_val: 0.5705 time: 0.2627s\n",
            "Epoch: 0051 loss_train: 1.3558 acc_train: 0.7214 loss_val: 1.5125 acc_val: 0.5798 time: 0.2643s\n",
            "Epoch: 0052 loss_train: 1.3474 acc_train: 0.6500 loss_val: 1.5027 acc_val: 0.5915 time: 0.2633s\n",
            "Epoch: 0053 loss_train: 1.3535 acc_train: 0.6857 loss_val: 1.4932 acc_val: 0.6016 time: 0.2633s\n",
            "Epoch: 0054 loss_train: 1.3082 acc_train: 0.6929 loss_val: 1.4838 acc_val: 0.6118 time: 0.2639s\n",
            "Epoch: 0055 loss_train: 1.2307 acc_train: 0.7143 loss_val: 1.4743 acc_val: 0.6211 time: 0.2642s\n",
            "Epoch: 0056 loss_train: 1.3412 acc_train: 0.6429 loss_val: 1.4649 acc_val: 0.6285 time: 0.2637s\n",
            "Epoch: 0057 loss_train: 1.2374 acc_train: 0.6857 loss_val: 1.4554 acc_val: 0.6328 time: 0.2639s\n",
            "Epoch: 0058 loss_train: 1.2770 acc_train: 0.7000 loss_val: 1.4460 acc_val: 0.6406 time: 0.2630s\n",
            "Epoch: 0059 loss_train: 1.1842 acc_train: 0.6929 loss_val: 1.4368 acc_val: 0.6484 time: 0.2634s\n",
            "Epoch: 0060 loss_train: 1.2333 acc_train: 0.7143 loss_val: 1.4280 acc_val: 0.6546 time: 0.2630s\n",
            "Epoch: 0061 loss_train: 1.1493 acc_train: 0.7429 loss_val: 1.4192 acc_val: 0.6636 time: 0.2632s\n",
            "Epoch: 0062 loss_train: 1.1907 acc_train: 0.7357 loss_val: 1.4104 acc_val: 0.6686 time: 0.2637s\n",
            "Epoch: 0063 loss_train: 1.2498 acc_train: 0.7071 loss_val: 1.4017 acc_val: 0.6690 time: 0.2642s\n",
            "Epoch: 0064 loss_train: 1.0896 acc_train: 0.7714 loss_val: 1.3929 acc_val: 0.6725 time: 0.2656s\n",
            "Epoch: 0065 loss_train: 1.1307 acc_train: 0.7500 loss_val: 1.3841 acc_val: 0.6748 time: 0.2638s\n",
            "Epoch: 0066 loss_train: 1.2309 acc_train: 0.7500 loss_val: 1.3752 acc_val: 0.6787 time: 0.2628s\n",
            "Epoch: 0067 loss_train: 1.1344 acc_train: 0.7500 loss_val: 1.3666 acc_val: 0.6826 time: 0.2632s\n",
            "Epoch: 0068 loss_train: 1.1866 acc_train: 0.7357 loss_val: 1.3579 acc_val: 0.6842 time: 0.2632s\n",
            "Epoch: 0069 loss_train: 1.0909 acc_train: 0.7571 loss_val: 1.3492 acc_val: 0.6854 time: 0.2638s\n",
            "Epoch: 0070 loss_train: 1.1816 acc_train: 0.7786 loss_val: 1.3406 acc_val: 0.6896 time: 0.2638s\n",
            "Epoch: 0071 loss_train: 1.1069 acc_train: 0.7643 loss_val: 1.3322 acc_val: 0.6935 time: 0.2642s\n",
            "Epoch: 0072 loss_train: 1.1062 acc_train: 0.7429 loss_val: 1.3237 acc_val: 0.6920 time: 0.2636s\n",
            "Epoch: 0073 loss_train: 1.1413 acc_train: 0.7286 loss_val: 1.3153 acc_val: 0.6935 time: 0.2631s\n",
            "Epoch: 0074 loss_train: 1.0669 acc_train: 0.7643 loss_val: 1.3073 acc_val: 0.6951 time: 0.2629s\n",
            "Epoch: 0075 loss_train: 1.0322 acc_train: 0.8071 loss_val: 1.2998 acc_val: 0.6951 time: 0.2635s\n",
            "Epoch: 0076 loss_train: 1.1213 acc_train: 0.7214 loss_val: 1.2923 acc_val: 0.6967 time: 0.2632s\n",
            "Epoch: 0077 loss_train: 1.0514 acc_train: 0.7643 loss_val: 1.2847 acc_val: 0.6978 time: 0.2631s\n",
            "Epoch: 0078 loss_train: 1.0167 acc_train: 0.8000 loss_val: 1.2772 acc_val: 0.6994 time: 0.2631s\n",
            "Epoch: 0079 loss_train: 1.0983 acc_train: 0.7500 loss_val: 1.2699 acc_val: 0.7009 time: 0.2637s\n",
            "Epoch: 0080 loss_train: 1.0328 acc_train: 0.7929 loss_val: 1.2625 acc_val: 0.7013 time: 0.2640s\n",
            "Epoch: 0081 loss_train: 1.0861 acc_train: 0.7786 loss_val: 1.2552 acc_val: 0.7021 time: 0.2640s\n",
            "Epoch: 0082 loss_train: 1.0463 acc_train: 0.7500 loss_val: 1.2480 acc_val: 0.7025 time: 0.2630s\n",
            "Epoch: 0083 loss_train: 1.0448 acc_train: 0.7500 loss_val: 1.2407 acc_val: 0.7048 time: 0.2630s\n",
            "Epoch: 0084 loss_train: 0.9975 acc_train: 0.8071 loss_val: 1.2335 acc_val: 0.7052 time: 0.2632s\n",
            "Epoch: 0085 loss_train: 0.9209 acc_train: 0.7786 loss_val: 1.2264 acc_val: 0.7052 time: 0.2636s\n",
            "Epoch: 0086 loss_train: 0.9429 acc_train: 0.7786 loss_val: 1.2194 acc_val: 0.7044 time: 0.2637s\n",
            "Epoch: 0087 loss_train: 0.9458 acc_train: 0.8571 loss_val: 1.2125 acc_val: 0.7044 time: 0.2639s\n",
            "Epoch: 0088 loss_train: 0.9079 acc_train: 0.7929 loss_val: 1.2059 acc_val: 0.7044 time: 0.2641s\n",
            "Epoch: 0089 loss_train: 0.9138 acc_train: 0.8143 loss_val: 1.1994 acc_val: 0.7048 time: 0.2638s\n",
            "Epoch: 0090 loss_train: 1.0282 acc_train: 0.7500 loss_val: 1.1931 acc_val: 0.7048 time: 0.2637s\n",
            "Epoch: 0091 loss_train: 1.0149 acc_train: 0.7571 loss_val: 1.1869 acc_val: 0.7056 time: 0.2635s\n",
            "Epoch: 0092 loss_train: 0.9581 acc_train: 0.8286 loss_val: 1.1807 acc_val: 0.7056 time: 0.2634s\n",
            "Epoch: 0093 loss_train: 0.9481 acc_train: 0.8214 loss_val: 1.1746 acc_val: 0.7064 time: 0.2640s\n",
            "Epoch: 0094 loss_train: 0.9745 acc_train: 0.7571 loss_val: 1.1686 acc_val: 0.7068 time: 0.2640s\n",
            "Epoch: 0095 loss_train: 0.8712 acc_train: 0.8000 loss_val: 1.1626 acc_val: 0.7068 time: 0.2651s\n",
            "Epoch: 0096 loss_train: 0.8678 acc_train: 0.8071 loss_val: 1.1565 acc_val: 0.7072 time: 0.2642s\n",
            "Epoch: 0097 loss_train: 0.8625 acc_train: 0.8286 loss_val: 1.1509 acc_val: 0.7079 time: 0.2650s\n",
            "Epoch: 0098 loss_train: 0.8909 acc_train: 0.7500 loss_val: 1.1454 acc_val: 0.7072 time: 0.2637s\n",
            "Epoch: 0099 loss_train: 0.8704 acc_train: 0.7929 loss_val: 1.1401 acc_val: 0.7060 time: 0.2637s\n",
            "Epoch: 0100 loss_train: 0.9442 acc_train: 0.7929 loss_val: 1.1346 acc_val: 0.7056 time: 0.2635s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 26.6284s\n",
            "Test set results: loss= 1.1346 accuracy= 0.7056\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question: (Your task)\n",
        "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
      ],
      "metadata": {
        "id": "n6Ox3fbTG7rc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urJ8Q-neDzHU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZhHh8k4DzJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmvJ46OfGlf2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}